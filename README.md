# Home_Sales

Hello! 

Thank you for visiting my SparkSQL project looking at data about home sales. This project aims to show my understanding and skill of pyspark, parquet, and caching methods which is helpful when using large datasets!

Queries include: 
  * *Average Home prices by year
  * *Average prices of homes with 3 bedrooms and 3 bathrooms organized by year built
  * *Average prices of homes with 3 beds, 3 baths, 2 stories with living space of at least 2000 sqft organized by year
  * *Average price of homes with values equal to or greater than $350k organized by descending view ratings, the higher the rating the better the view
  * *Time measurements of queries after tables were cached and again after they were parquet.

Results: Control query ran for 0.25 seconds, Cahced query ran for .16 seconds, Parquet query ran for 0.0 seconds.

All code was written by me using the course materials and the use of the class AI tool and Claude AI to troubleshoot pyspark installation and set up issues.
